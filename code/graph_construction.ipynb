{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgcn import TextGCN\n",
    "import pandas as pd\n",
    "import torch\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data\n",
    "dev=pd.read_csv('../data/dev.csv')\n",
    "train=pd.read_csv('../data/train.csv')\n",
    "test=pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations,numbers and emojis from text\n",
    "import re \n",
    "\n",
    "#Removing punctuations\n",
    "train['text']=train['text'].str.replace('[^\\w\\s]','')\n",
    "dev['text']=dev['text'].str.replace('[^\\w\\s]','')\n",
    "test['text']=test['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#Removing numbers\n",
    "train['text']=train['text'].str.replace('\\d+','')\n",
    "dev['text']=dev['text'].str.replace('\\d+','')\n",
    "test['text']=test['text'].str.replace('\\d+','')\n",
    "\n",
    "#Removing usernames(@user)\n",
    "train['text']=train['text'].str.replace('@\\w+','')\n",
    "\n",
    "#Removing emojis\n",
    "train['text']=train['text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "dev['text']=dev['text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "test['text']=test['text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "\n",
    "#Removing urls\n",
    "train['text']=train['text'].str.replace(r'^https?:\\/\\/.*[\\r\\n]*', '', flags=re.MULTILINE)\n",
    "dev['text']=dev['text'].str.replace(r'^https?:\\/\\/.*[\\r\\n]*', '', flags=re.MULTILINE)\n",
    "test['text']=test['text'].str.replace(r'^https?:\\/\\/.*[\\r\\n]*', '', flags=re.MULTILINE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "#Read TamilStopWords.txt file and append each line to stopwords list\n",
    "with open('TamilStopWords.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        stopwords.append(line.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabi(train,test,dev):\n",
    "    #Remove stop words of tamil language\n",
    "    vocab={}\n",
    "    for i in train['text']:\n",
    "        for j in i.split():\n",
    "            if j not in vocab:\n",
    "                vocab[j]=1\n",
    "            else:\n",
    "                vocab[j]+=1\n",
    "    for i in test['text']:\n",
    "        for j in i.split():\n",
    "            if j not in vocab:\n",
    "                vocab[j]=1\n",
    "            else:\n",
    "                vocab[j]+=1\n",
    "    for i in dev['text']:\n",
    "        for j in i.split():\n",
    "            if j not in vocab:\n",
    "                vocab[j]=1\n",
    "            else:\n",
    "                vocab[j]+=1\n",
    "    vocabul={}\n",
    "    for i in vocab:\n",
    "        if i not in stopwords:\n",
    "            vocabul[i]=vocab[i]\n",
    "    vocab=vocabul\n",
    "\n",
    "    for i in train['text']:\n",
    "        sentence=''\n",
    "        for j in i.split():\n",
    "            if j in vocab:\n",
    "                sentence+=' '+j\n",
    "        train['text'].replace(i,sentence,inplace=True)\n",
    "    for i in test['text']:\n",
    "        sentence=''\n",
    "        for j in i.split():\n",
    "            if j not in vocab:\n",
    "                pass\n",
    "            else:\n",
    "                sentence+=' '+j\n",
    "        test['text'].replace(i,sentence,inplace=True)\n",
    "    for i in dev['text']:\n",
    "        sentence=''\n",
    "        for j in i.split():\n",
    "            if j not in vocab:\n",
    "                pass\n",
    "            else:\n",
    "                sentence+=' '+j\n",
    "        dev['text'].replace(i,sentence,inplace=True)\n",
    "    vocabulary={}\n",
    "    i=0\n",
    "    for j in vocab:\n",
    "        vocabulary[j]=i\n",
    "        i+=1\n",
    "    return vocabulary\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=vocabi(train,test,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pmi(data1,data2,data3,vocab):\n",
    "    import math\n",
    "    sliding_window=5\n",
    "    pmi={}\n",
    "    number_of_windows=0\n",
    "    count_of_words={}\n",
    "    count_of_word_pairs={}\n",
    "    #Loop over all the documents in data1 and data2\n",
    "    for i in data1:\n",
    "        words=i.split()\n",
    "        for j in range(len(words)):\n",
    "            if vocab[words[j]] not in count_of_words:\n",
    "                count_of_words[vocab[words[j]]]=1\n",
    "            else:\n",
    "                count_of_words[vocab[words[j]]]+=1\n",
    "            for k in range(j+1,min(j+sliding_window,len(words))):\n",
    "                a=min(vocab[words[j]],vocab[words[k]])\n",
    "                b=max(vocab[words[j]],vocab[words[k]])\n",
    "                if (a,b) not in count_of_word_pairs:\n",
    "                    count_of_word_pairs[(a,b)]=1\n",
    "                else:\n",
    "                    count_of_word_pairs[(a,b)]+=1\n",
    "        number_of_windows+=1\n",
    "    for i in data2:\n",
    "        words=i.split()\n",
    "        for j in range(len(words)):\n",
    "            if vocab[words[j]] not in count_of_words:\n",
    "                count_of_words[vocab[words[j]]]=1\n",
    "            else:\n",
    "                count_of_words[vocab[words[j]]]+=1\n",
    "            for k in range(j+1,min(j+sliding_window,len(words))):\n",
    "                a=min(vocab[words[j]],vocab[words[k]])\n",
    "                b=max(vocab[words[j]],vocab[words[k]])\n",
    "                if (a,b) not in count_of_word_pairs:\n",
    "                    count_of_word_pairs[(a,b)]=1\n",
    "                else:\n",
    "                    count_of_word_pairs[(a,b)]+=1\n",
    "        number_of_windows+=1\n",
    "    for i in data3:\n",
    "        words=i.split()\n",
    "        for j in range(len(words)):\n",
    "            if vocab[words[j]] not in count_of_words:\n",
    "                count_of_words[vocab[words[j]]]=1\n",
    "            else:\n",
    "                count_of_words[vocab[words[j]]]+=1\n",
    "            for k in range(j+1,min(j+sliding_window,len(words))):\n",
    "                a=min(vocab[words[j]],vocab[words[k]])\n",
    "                b=max(vocab[words[j]],vocab[words[k]])\n",
    "                if (a,b) not in count_of_word_pairs:\n",
    "                    count_of_word_pairs[(a,b)]=1\n",
    "                else:\n",
    "                    count_of_word_pairs[(a,b)]+=1\n",
    "        number_of_windows+=1\n",
    "    \n",
    "    for i in count_of_word_pairs:\n",
    "        pmi[i]=math.log((count_of_word_pairs[(i[0],i[1])]*number_of_windows)/(count_of_words[i[0]]*count_of_words[i[1]]))\n",
    "        pmi[(i[1],i[0])]=pmi[i]\n",
    "    return pmi\n",
    "\n",
    "def find_tfidf(data1,data2,data3,vocab):\n",
    "    import math\n",
    "    tfidf={}\n",
    "    number_of_documents=0\n",
    "    count_of_words={}\n",
    "    max_count_of_words={}\n",
    "    tf={}\n",
    "    idf={}\n",
    "    count=0\n",
    "    for i in data1:\n",
    "        max_count_of_words[len(vocab)+count]=0\n",
    "        words=i.split()\n",
    "        for j in range(len(words)):\n",
    "            if vocab[words[j]] not in count_of_words:\n",
    "                count_of_words[vocab[words[j]]]=1\n",
    "                max_count_of_words[len(vocab)+count]=max(max_count_of_words[len(vocab)+count],1)\n",
    "            else:\n",
    "                count_of_words[vocab[words[j]]]+=1\n",
    "                max_count_of_words[len(vocab)+count]=max(max_count_of_words[len(vocab)+count],count_of_words[vocab[words[j]]])\n",
    "        number_of_documents+=1\n",
    "        count+=1\n",
    "    for i in data2:\n",
    "        max_count_of_words[len(vocab)+count]=0\n",
    "        words=i.split()\n",
    "        for j in range(len(words)):\n",
    "            if vocab[words[j]] not in count_of_words:\n",
    "                count_of_words[vocab[words[j]]]=1\n",
    "                max_count_of_words[len(vocab)+count]=max(max_count_of_words[len(vocab)+count],1)\n",
    "            else:\n",
    "                count_of_words[vocab[words[j]]]+=1\n",
    "                max_count_of_words[len(vocab)+count]=max(max_count_of_words[len(vocab)+count],count_of_words[vocab[words[j]]])\n",
    "        number_of_documents+=1\n",
    "        count+=1\n",
    "    for i in data3:\n",
    "        max_count_of_words[len(vocab)+count]=0\n",
    "        words=i.split()\n",
    "        for j in range(len(words)):\n",
    "            if vocab[words[j]] not in count_of_words:\n",
    "                count_of_words[vocab[words[j]]]=1\n",
    "                max_count_of_words[len(vocab)+count]=max(max_count_of_words[len(vocab)+count],1)\n",
    "            else:\n",
    "                count_of_words[vocab[words[j]]]+=1\n",
    "                max_count_of_words[len(vocab)+count]=max(max_count_of_words[len(vocab)+count],count_of_words[vocab[words[j]]])\n",
    "        number_of_documents+=1\n",
    "        count+=1\n",
    "    \n",
    "    for i in vocab:\n",
    "        count=0\n",
    "        for j in range(len(data1)):\n",
    "            word=data1[j].split()\n",
    "            for k in word:\n",
    "                if k==i:\n",
    "                    if (vocab[k],len(vocab)+j) not in tf:\n",
    "                        tf[(vocab[k],len(vocab)+j)]=1\n",
    "                        count+=1\n",
    "                    else:\n",
    "                        tf[(vocab[k],len(vocab)+j)]+=1\n",
    "                        count+=1\n",
    "        for j in range(len(data2)):\n",
    "            word=data2[j].split()\n",
    "            for k in word:\n",
    "                if k==i:\n",
    "                    if (vocab[k],len(vocab)+j+len(data1)) not in tf:\n",
    "                        tf[(vocab[k],len(vocab)+j+len(data1))]=1\n",
    "                        count+=1\n",
    "                    else:\n",
    "                        tf[(vocab[k],len(vocab)+j+len(data1))]+=1\n",
    "                        count+=1\n",
    "        for j in range(len(data3)):\n",
    "            word=data3[j].split()\n",
    "            for k in word:\n",
    "                if k==i:\n",
    "                    if (vocab[k],len(vocab)+j+len(data1)+len(data2)) not in tf:\n",
    "                        tf[(vocab[k],len(vocab)+j+len(data1)+len(data2))]=1\n",
    "                        count+=1\n",
    "                    else:\n",
    "                        tf[(vocab[k],len(vocab)+j+len(data1)+len(data2))]+=1\n",
    "                        count+=1\n",
    "        if count!=0:\n",
    "            idf[vocab[i]]=math.log(number_of_documents/count)\n",
    "\n",
    "    for (a,b) in tf:\n",
    "        tfidf[(a,b)]=(tf[(a,b)]*idf[a])/max_count_of_words[b]\n",
    "        tfidf[(b,a)]=tfidf[(a,b)]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(data1,data2,data3,vocab):\n",
    "    #Find number of unique words in data\n",
    "    matrix=torch.zeros(len(vocab)+len(data1)+len(data2)+len(data3),len(vocab)+len(data1)+len(data2)+len(data3)).to(device)\n",
    "    pmi=find_pmi(data1,data2,data3,vocab)\n",
    "    tfidf=find_tfidf(data1,data2,data3,vocab)\n",
    "    for i in range(len(vocab)+len(data1)+len(data2)+len(data3)):\n",
    "        matrix[i][i]=1\n",
    "    for i in pmi:\n",
    "            matrix[i[0]][i[1]]=pmi[i]\n",
    "    for i in tfidf:\n",
    "        matrix[i[1]][i[0]]=tfidf[i]\n",
    "\n",
    "    #Write the length of data1 and data2 in a file\n",
    "    f=open('length.txt','w')\n",
    "    f.write(str(len(data1)))\n",
    "    f.write('\\n')\n",
    "    f.write(str(len(data2)))\n",
    "    f.write('\\n')\n",
    "    f.write(str(len(data3)))\n",
    "    f.close()\n",
    "    return matrix.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=create_matrix(train['text'],dev['text'],test['text'],vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find D\n",
    "D=torch.zeros(len(vocab)+len(train['text'])+len(dev['text'])+len(test['text']),len(vocab)+len(train['text'])+len(dev['text'])+len(test['text'])).to(device)\n",
    "for i in range(len(vocab)+len(train['text'])+len(dev['text'])+len(test['text'])):\n",
    "    D[i][i]=torch.sum(adj[i])\n",
    "D=D.to(device)\n",
    "#Make matrix inverse of D\n",
    "D=torch.inverse(D)\n",
    "D=torch.pow(D,0.5)\n",
    "adj=torch.mm(torch.mm(D,adj),D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(adj,'../adj.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
